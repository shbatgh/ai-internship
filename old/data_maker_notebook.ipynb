{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/envs/YOLOv5/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_bbox = []\n",
    "one_cell_indices = []\n",
    "\n",
    "for i in range(len(boxes_list)):\n",
    "    if len(boxes_list[i]) == 1:\n",
    "        cell_bbox.append(boxes_list[i])\n",
    "        one_cell_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[438.,   2., 471.,  35.]])\n"
     ]
    }
   ],
   "source": [
    "print(boxes_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_cell_images = []\n",
    "\n",
    "j = 0\n",
    "\n",
    "for i in range(len(images_path)):\n",
    "    temp_path = int(images_path[i][12:-4])\n",
    "    if temp_path == one_cell_indices[j]:\n",
    "        one_cell_images.append(images[i])\n",
    "        j += 1\n",
    "        if j == len(one_cell_indices):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_tuple(tensor):\n",
    "    return tuple(tensor.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182 182\n"
     ]
    }
   ],
   "source": [
    "print(len(cell_bbox), len(one_cell_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = []\n",
    "\n",
    "for i in range(len(one_cell_images)):\n",
    "    cropped_images.append(Image.fromarray(one_cell_images[i]).crop(tuple(cell_bbox[i].tolist()[0])))\n",
    "    #print() #.crop(cell_bbox[i].tolist())\n",
    "    #print(cell_bbox[i].tolist(), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_drawn_boxes(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of sample single-cell images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images.insert(0, cropped_images[0])\n",
    "cropped_images.insert(0, cropped_images[2])\n",
    "cropped_images.insert(0, cropped_images[3])\n",
    "cropped_images.insert(0, cropped_images[4])\n",
    "cropped_images.insert(0, cropped_images[6])\n",
    "cropped_images.insert(0, cropped_images[7])\n",
    "cropped_images.insert(0, cropped_images[8])\n",
    "cropped_images.insert(0, cropped_images[9])\n",
    "cropped_images.insert(0, cropped_images[10])\n",
    "cropped_images.insert(0, cropped_images[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cropped_images) - 10):\n",
    "    cropped_images.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing bounding box on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_test_image = transform(cropped_images[0])\n",
    "bb_test_box = np.array([[5, 10, 15, 20]])\n",
    "bb_test_box = torch.Tensor(bb_test_box)\n",
    "\n",
    "print(type(boxes_list[0]))\n",
    "print(type(bb_test_box))\n",
    "print(boxes_list[0])\n",
    "print(bb_test_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACEAAAAiCAIAAABeEieWAAAE6ElEQVR4nI2WW5LbKBSGEQLd7e5FzR5SlVnOZDPJbGGWlNiWZBAgEPPw2VRXp6o7PLgsC5/LfzlQKaWqqoox1s/VNI1SSimltdZa13VdVZVSSkophKiqSgiRc7bWppRSSs65fd9DCPu+CyFSSjlntrEqKSVRCKeUapqmruuu65rnklJqrUnD//fn8t4757bnijGmlEhwHEfOWQihiE7hXddprdu27bqubVsy8Utd1xQhpUwpHcex77tzzjnnvV+Whbfe+5QSmdgJBory+77vuq7rur7v+74HMfKRprQihIgx7vseY7TWGmPatl2WpWkaay3dOOdII4RQpepxHNu2HYZhHMe+7wnaNE3btiBGo0opWAkhhBDu9/s0TdR3vV7ZZq3NOW/bBmiKEOM4juM4DMPpdCLBMAxvy6czQtR1Ddz7vp9Op/v9Tll1XS/LUlUV5eecnXM5Z0WCruumaTqdTq+vr+To+76qqqZpyNE+V1HBcRwppW3bxnFc1xUYYaWu6+M40FiMUUHAOI7TNJ3PZ77o54Iqvg/DoJQahgF9H8dxHAcokRUbFDkJIfZ9zzk/fDBN0zAMwzBM0zSOI9GFEBQFB8dxoF3K5EeUJqVEP0IIHlNKxTSqaZrCOeyBLBUV01Hj2wTgXpSWc6aC4zhQHdaJMariL3b/8+2b+HD9++NHyURQKWXf95Sfc44xxhj9cznnVFVVoFHXddu2HycAYq11SokWwaqqqq7rwOd8PjvnzueztXYYhm3bFBUxIQpXHywmW4wRzgtcOWdgDyHA68vLC5ZUDIa3I+zTPuq6LugLIaCHTEjudDpt22aMmabJGPPIAXYMzo8XiNN0SgkRM6CYacVDuGIYBonCMMvvOf778uX3NCmlEEKMMefM1GLIA93bTA81sZtMMcZ34f76/v3dL1VVASxBlVJIvHgIbxXDP9SGlkMI3vtPscI6ZdEN/HM68flWRA8+aMI592kOfFBkorUuaiRZgZE9OWeVnss513Xdn+SglXL0lqwACEMcVuhIQSBw/QlWWmusRx9EpxW0s++7MWbbtnVdrbXOOSWE4GSG+b+/fn15eWF8cUAxzRjvnB/IFCgoXzzHuHPOGHN/rnVd13VVuAmNw1uMkUBATI1gwnetNeSzoYxY770xxlo7z/Ptdrter+u6OuckJZTLRDF8KZBAfGHcsqiMMc7N4X6/G2Nut9vlcvn58+f9fn/MEv7Pfecdk0UY4E7iMnGhgckaQljXFQ5+/fp1uVxut9u6rsYY7/3jAlB4Q9eFT/E0XXkbQqB2KSVEOufmeYaAZVmu1+vlciEBly4FDQV0HklAPt5iVYzNIxQ655ZlWdd1nufr9TrPM48wb62NMT5ONwJx/OKgEEJd1ykl+N/3vUxAyEP+xhhqJ8HtdluWxRgD+fhRIQwq0lpv28Yn98cQAtxQCoOAPpxz1lqqLh1Aw7ZtnLKU/mjce980jXOOK0Q53YqiEDRg0hZWWJaF8ud5BiJjDLfsMrUePs85G2PKqCjC5y0GLK723scY4XOeZzrAcdyA+SNQC2ZimV8ccOJ5QjCMpZTc3pATl0zGDzYmOh3EGEMIRCsnhfLek7CYHNqxLoPSe1+uHeiVNJgO0CDg7cRFk1JKVYTEGUCUEMK2bUwtDhytNYZAcgw7tllrma/AW9otp/3/+gz71ModjNIAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=33x34>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bb_test = draw_bounding_boxes(bb_test_image, bb_test_box, width=5, colors=\"red\", fill=\"False\")\n",
    "\n",
    "bb_test = torchvision.transforms.ToPILImage()(bb_test)\n",
    "\n",
    "bb_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_bbox.insert(0, cell_bbox[0])\n",
    "cell_bbox.insert(0, cell_bbox[2])\n",
    "cell_bbox.insert(0, cell_bbox[3])\n",
    "cell_bbox.insert(0, cell_bbox[4])\n",
    "cell_bbox.insert(0, cell_bbox[6])\n",
    "cell_bbox.insert(0, cell_bbox[7])\n",
    "cell_bbox.insert(0, cell_bbox[8])\n",
    "cell_bbox.insert(0, cell_bbox[9])\n",
    "cell_bbox.insert(0, cell_bbox[10])\n",
    "cell_bbox.insert(0, cell_bbox[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cell_bbox[0]\n",
    "t[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cell_bbox)):\n",
    "    for j in range(len(cell_bbox[i])):\n",
    "        cell_bbox[i][0, j] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cell_bbox) - 10):\n",
    "    cell_bbox.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in range(len(one_cell_images)):\n",
    "    one_cell_images[image] = Image.fromarray(one_cell_images[image])\n",
    "    one_cell_images[image].convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "\n",
    "for i in range(len(cropped_images)):\n",
    "    cropped_images[i] = np.array(cropped_images[i]) > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_to_alpha(img):\n",
    "    img = Image.fromarray(img)\n",
    "    img = img.convert(\"RGBA\")\n",
    "    datas = img.getdata()\n",
    "\n",
    "    newData = []\n",
    "\n",
    "    for item in datas:\n",
    "        if item[0] == 0 and item[1] == 0 and item[2] == 0:\n",
    "            newData.append((255, 255, 255, 0))\n",
    "        else:\n",
    "            newData.append(item)\n",
    "\n",
    "    img.putdata(newData)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cropped_images)):\n",
    "    cropped_images[i] = black_to_alpha(np.array(cropped_images[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images = []\n",
    "one_cell_bboxes = []\n",
    "\n",
    "c = 0\n",
    "for j in range(2000):\n",
    "    test = torch.empty(0, 4)\n",
    "    temp_bbox = torch.empty((35))\n",
    "    test_image = Image.new('RGB', (696, 520), color=(0, 0, 0, 0))\n",
    "    for i in range(random.randint(35, 50)):\n",
    "        rand_image = random.randint(0, len(cropped_images) - 1)\n",
    "        box = cell_bbox[rand_image]\n",
    "        #box = boxes_list[one_cell_indices[rand_image]]\n",
    "        #temp_bbox = torch.cat((temp_bbox, box))\n",
    "        test = torch.cat((test, box))\n",
    "        #one_cell_bboxes.append(boxes_list[one_cell_indices[rand_image]])\n",
    "        #temp_background = Image.new('RGBA', (696, 520), color=(0, 0, 0, 0))\n",
    "        randx = random.randint(0, 660)\n",
    "        randy = random.randint(0, 480)\n",
    "        #temp_background.paste(cropped_images[5], (randx, randy))\n",
    "        test_image.paste(cropped_images[rand_image], (randx, randy), Image.fromarray(np.array(cropped_images[rand_image])[:,:,3]))\n",
    "\n",
    "    #test_image.save(\"data/images/{number}.png\".format(number=c), \"PNG\")\n",
    "    generated_images.append(test_image) \n",
    "    one_cell_bboxes.append(test)\n",
    "    c += 1\n",
    "    #plt.imshow(test_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(randx, randy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box[0,0] = randx\n",
    "box[0, 1] = randy\n",
    "box[0, 2] = randx + cell_bbox[0][0,2] - cell_bbox[0][0,0]\n",
    "box[0, 3] = randy + cell_bbox[0][0, 1] - cell_bbox[0][0, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cell_bbox[0][0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_bbox[0][0, 3] - cell_bbox[0][0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_images[-1].save(\"test.png\", \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = read_image(\"test.png\")\n",
    "tt = draw_bounding_boxes(ttt, box, colors=\"red\")\n",
    "show(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_bbox[0][0,0] + cell_bbox[0][0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imag = cropped_images[0].convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawn_boxes = draw_bounding_boxes(transform(test_imag), cell_bbox[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor_to_tuple(cell_bbox[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "y = 10\n",
    "test_box = torch.tensor([x, y, x+10, y-5])\n",
    "print(test_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(drawn_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(generated_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = cell_bbox[0]\n",
    "#box = [330, 190, 660, 355]\n",
    "#box = torch.tensor(box)\n",
    "#box = box.unsqueeze(0)\n",
    "img = transform(Image.fromarray(cropped_images[0]).convert(\"L\"))\n",
    "\n",
    "img = draw_bounding_boxes(img, box, width=5, colors=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.ToPILImage()(img)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_cell_bboxes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_cell_bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_tensor = transforms.ToTensor()\n",
    "testy = read_image(\"cell_images/0.png\")\n",
    "googa = draw_bounding_boxes(testy, boxes_list[1], colors=\"red\")\n",
    "show(googa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imag = Image.open(\"data/images/train/0.png\")\n",
    "imag = imag.convert('RGB')\n",
    "imag.save(\"imag.png\", \"PNG\")\n",
    "\n",
    "imag = read_image(\"imag.png\")\n",
    "drawn_boxes = draw_bounding_boxes(imag, one_cell_bboxes[0], colors=\"red\")\n",
    "show(drawn_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(generated_images)):\n",
    "    generated_images[i].convert(\"L\")\n",
    "    generated_images[i].save(\"temp2/{number}.png\".format(number=i), \"PNG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(generated_images[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_drawn_boxes(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generated_images[0] = np.array(generated_images[0])\n",
    "#generated_images[0] = generated_images[0][:][:3]\n",
    "#print(np.array(generated_images[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generated_images[0].convert(\"RGB\")\n",
    "#generated_images[0].save(\"imag.png\", \"PNG\")\n",
    "#imag = read_image(\"imag.png\")\n",
    "#drawn_boxes = draw_bounding_boxes(imag, one_cell_bboxes, colors=\"red\")\n",
    "#show(drawn_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for box in range(len(one_cell_bboxes)):\n",
    "#    one_cell_bboxes[box] = normalize(one_cell_bboxes[box], p=1.0, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_box_info_to_file(one_cell_bboxes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('YOLOv5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59c369f0353834940d06bc12af3d8934cf452b2253cb1a3d912f389c93b40c3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
