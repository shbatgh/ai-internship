{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/envs/YOLOv5/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old Cell Bounding Box Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cell_bbox = []\n",
    "one_cell_indices = []\n",
    "\n",
    "for i in range(len(boxes_list)):\n",
    "    if len(boxes_list[i]) == 1:\n",
    "        temp_cell_bbox.append(boxes_list[i])\n",
    "        one_cell_indices.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 12 different one cell samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_cell_images = []\n",
    "\n",
    "j = 0\n",
    "\n",
    "for i in range(len(images_path)):\n",
    "    temp_path = int(images_path[i][12:-4])\n",
    "    if temp_path == one_cell_indices[j]:\n",
    "        one_cell_images.append(images[i])\n",
    "        j += 1\n",
    "        if j == len(one_cell_indices):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_tuple(tensor):\n",
    "    return tuple(tensor.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(cell_bbox), len(one_cell_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = []\n",
    "\n",
    "for i in range(len(one_cell_images)):\n",
    "    cropped_images.append(Image.fromarray(one_cell_images[i]).crop(tuple(temp_cell_bbox[i].tolist()[0])))\n",
    "    #print() #.crop(cell_bbox[i].tolist())\n",
    "    #print(cell_bbox[i].tolist(), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of sample single-cell images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images.insert(0, cropped_images[0])\n",
    "cropped_images.insert(0, cropped_images[2])\n",
    "cropped_images.insert(0, cropped_images[3])\n",
    "cropped_images.insert(0, cropped_images[4])\n",
    "cropped_images.insert(0, cropped_images[6])\n",
    "cropped_images.insert(0, cropped_images[7])\n",
    "cropped_images.insert(0, cropped_images[8])\n",
    "cropped_images.insert(0, cropped_images[9])\n",
    "cropped_images.insert(0, cropped_images[10])\n",
    "cropped_images.insert(0, cropped_images[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cropped_images) - 10):\n",
    "    cropped_images.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(cropped_images[0].size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make it so that cell_bbox is a list of the bounding boxes where the numbers correspond to the corners of its corresponding cropped_image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_bbox = np.empty([10,4])\n",
    "\n",
    "for i in range(len(cropped_images)):\n",
    "    cell_bbox[i] = [0, 0, cropped_images[i].size[0]-1, cropped_images[i].size[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. 32. 33.]\n",
      " [ 0.  0. 32. 33.]\n",
      " [ 0.  0. 32. 33.]\n",
      " [ 0.  0. 32. 33.]\n",
      " [ 0.  0. 32. 33.]\n",
      " [ 0.  0. 32. 33.]\n",
      " [ 0.  0. 70. 48.]\n",
      " [ 0.  0. 70. 48.]\n",
      " [ 0.  0. 70. 48.]\n",
      " [ 0.  0. 32. 32.]]\n"
     ]
    }
   ],
   "source": [
    "print(cell_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing bounding box on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cell_bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[438.,   2., 471.,  35.]])\n",
      "tensor([[ 0.,  0., 32., 33.]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5512/2569431714.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  bb_test_box = torch.tensor([cell_bbox[0]])\n"
     ]
    }
   ],
   "source": [
    "bb_test_image = transform(cropped_images[0])\n",
    "# Set bb_test_box to cell_bbox[0] converted to a pytorch tensor\n",
    "bb_test_box = torch.tensor([cell_bbox[0]])\n",
    "bb_test_box = torch.Tensor(bb_test_box)\n",
    "\n",
    "print(type(boxes_list[0]))\n",
    "print(type(bb_test_box))\n",
    "print(boxes_list[0])\n",
    "print(bb_test_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACEAAAAiCAIAAABeEieWAAAD1klEQVR4nHWWy7brKAxEedn4nA/t/x8kMSAePdhJNTd3tQZexDEqqVQS+H9C8N7POb33IYQQQkophBA/5r333rNwm7XW5pxzTjMbY/TexxjOubXWWmv/OPEbd8Lw3h/HkVJKKeFdeGwbY+SccW1mrbUQAqhrrRCCwJxzaQc4jiPGmFICYH9DZuGT9FprjGFmZtZ7L6XUWkMIvfc55xgDYkgghY8dHzvPUxjgxRh5guGcw9EYo7XWWksfa62ZWQjBzIBxzr1DluvzY7CkzeR6HAc8eO977733WquZERzfl1JaaxTsXQ/sPM+c83me13UpG/bIu9bAkEprrdZKWCGE+76dcyGEUspay8zWWinGmHM+jiPnfF3X7++vSFOplJCChWtEVWstpUgaMD/nJI45Z8IdAD8/PznnnLNcs4E1mRFvjBHZwCqokpNUiy7e6UMUjIFBuYTBZsIPIay1eKnGcs7xRtobH0viGgDS2iMCDFQA5py4896TwR4BslbrjDG+W1oA4nTPnZfkIafsYhxIC6iOBkoihBYhup0T1mCAquT8Zsdx4P26Ljq/tXaep5klHLETj/CrSDU/1lq9d3qTVtcc4l+EN8agB0Ays8TX7i/7AoZl7/0YQ22oDyCc4GKM13WRRM651hqQMPSNMXCNbPCFO/QjMO3iY9bs2ocevZzQAHu0CCEQ9V4M5URViJrP0CtP2NuHUJKKyaP3zihkznwVVj/B48DAVBUI0MHjvQ982jcDUv21865gxeF+LkkIu/acc2+FMPc5DCBt37NXdedEue5kqq4q7bvm+5FJ9XjuZ/AuHnWuIAlIRSVW8JJUT1uCoQN8l7+6dT+rNQ6cc+Kg1tpa4yAxs+CcG38ZotwbTULS4JOmNQKYspy7WCmllJI0M/ZkNfhEq4arJMQT73CiEXLf9+v1ej6fpZT3vKIe/9fn+1owrBUNtYSi+76fz+fj8eCnmaX39SQl9ZHbxqJ6UD2hQUlYGuCwX2t9PB7P5/P1ekFX7z19Rbpfv/amkxBRhPIA475vylBKEUtcJ1pr6atxNCL31uMNopTMUAdFLqVAEWAqO7e6/04xtmlC7JWX6pE4TUOpiR1yKPUO8D5rpSUzizHy5GoUY9RkxMxMxMIDgQOAqS0kpUQGtCEAuo7sQ3eMQVeSkJSKXzBUdlSgvUnk1Frdn2NVhwSzmn81kXaM8jEkoAPinYdab5c/jnrv53n6zx1eeoVoFQMjAx1W+whIXIHV6oLUxeI4DvTKN7pKqye4KhK+Rv3eCe8+/9Jo712XLo1YGoIUca17u+a57uo7zL99BQTr9ujShQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=33x34>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bb_test = draw_bounding_boxes(bb_test_image, bb_test_box, width=0, colors=\"red\", fill=\"False\")\n",
    "\n",
    "bb_test = torchvision.transforms.ToPILImage()(bb_test)\n",
    "\n",
    "bb_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "\n",
    "for i in range(len(cropped_images)):\n",
    "    cropped_images[i] = np.array(cropped_images[i]) > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_to_alpha(img):\n",
    "    img = Image.fromarray(img)\n",
    "    img = img.convert(\"RGBA\")\n",
    "    datas = img.getdata()\n",
    "\n",
    "    newData = []\n",
    "\n",
    "    for item in datas:\n",
    "        if item[0] == 0 and item[1] == 0 and item[2] == 0:\n",
    "            newData.append((255, 255, 255, 0))\n",
    "        else:\n",
    "            newData.append(item)\n",
    "\n",
    "    img.putdata(newData)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(cropped_images)):\n",
    "    #cropped_images[i] = black_to_alpha(np.array(cropped_images[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def flip_image(image, box):\\n    # Flip the image\\n    image = transforms.functional.hflip(image)\\n    # Flip the bounding box\\n    box[:, 0] = 1 - box[:, 0]\\n    # Swap the x_min and x_max values\\n    box[:, [0, 2]] = box[:, [2, 0]]\\n    return image, box'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write an artificial data augmentation functions to randomly flip the image and bounding box\n",
    "\"\"\"def flip_image(image, box):\n",
    "    # Flip the image\n",
    "    image = transforms.functional.hflip(image)\n",
    "    # Flip the bounding box\n",
    "    box[:, 0] = 1 - box[:, 0]\n",
    "    # Swap the x_min and x_max values\n",
    "    box[:, [0, 2]] = box[:, [2, 0]]\n",
    "    return image, box\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cropped_images)):\n",
    "    cropped_images[i] = Image.fromarray(cropped_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all cropped_images to RGB\n",
    "for i in range(len(cropped_images)):\n",
    "    cropped_images[i] = cropped_images[i].convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all cropped_images to pytorch tensors\n",
    "for i in range(len(cropped_images)):\n",
    "    cropped_images[i] = transform(cropped_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all cell_bbox to pytorch tensors\n",
    "for i in range(len(cell_bbox)):\n",
    "    cell_bbox[i] = torch.from_numpy(cell_bbox[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0., 32., 33.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor([cell_bbox[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0. 32. 33.]\n"
     ]
    }
   ],
   "source": [
    "print(cell_bbox[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0., 32., 33.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor(cell_bbox.tolist())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_bbox = torch.Tensor(cell_bbox.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0., 32., 33.]])\n"
     ]
    }
   ],
   "source": [
    "print(cell_bbox[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    cell_bbox[i].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0., 32., 33.])\n"
     ]
    }
   ],
   "source": [
    "print(cell_bbox[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0., 32., 33.]])\n"
     ]
    }
   ],
   "source": [
    "box = cell_bbox[0].unsqueeze(0)\n",
    "print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACEAAAAiCAIAAABeEieWAAAAqklEQVR4nNWXSw7AIAhEgfvf2S5MWuMHsc6YOMtKecDYVEX40mlESsl7X+cZhhF+6iVSZ20pewRTL/wD+KTv0WZ2B2OQvKXaWm20AMQYHNBi8LNqMUTGKy4jt2IMMyqpcDwvdb8fZxmRf8Aug62DDOq46H2o6v1+ZAuIjNdjFqPcRBRGtUvxDO9cAvlEukmQfYyqtEjQDkAg591pZZ1ZLXWzdTfIcnqKl/IAusgwTWQFtkYAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=33x34>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "caca = torchvision.transforms.ToPILImage()(cropped_images[0])\n",
    "caca.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAAxCAIAAAAHqSOUAAABNElEQVR4nO2ZWxKDIAxFI9OlNqtys/1gBq2GiDwkue356lRnyOVghHZ5k8a6rup1QzBz+hyU+xxFou9qs6l8RYqkmjVXHonB5FQeRe1Zzt3CeyTCW4GRYyoAUfQTrjBEMfOWCiNSBHoFIokiZFdIouLOHdcVDOmIFZCWXwLKVeKfyjalv1v4JewjwgDi6uAmnL8CAMHV2UrIXXDN5sppMLHscHmHR47Pla9guWqFbuErmIjcA10EU4rMdnbjwfTytPeV2WCXhV28hZnZbDYF4T8RESNH5sIpLt0x+TJW6ioxUVr5zN7e3c6Sdmvc17g6elExjzUnkSd11Y1Veb56Jlj1KEZXYOOs3e6Be0b0wy6rwJCrjqu6yRV10tX9KZ3mami/aXVFZboefnd3SBXJZZuyF/kAvwVYXvLiyfAAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=71x49>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot cropped_images[0] with bounding box\n",
    "bb_test_image = cropped_images[7]\n",
    "bb_test_box = cell_bbox[7].unsqueeze(0)\n",
    "bb_test = draw_bounding_boxes(bb_test_image, bb_test_box, width=0, colors=\"red\", fill=\"False\")\n",
    "bb_test = torchvision.transforms.ToPILImage()(bb_test)\n",
    "bb_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = draw_bounding_boxes(cropped_images[0], box, colors=\"red\", fill=\"False\")\n",
    "test_image = torchvision.transforms.ToPILImage()(test_image)\n",
    "\n",
    "test_image.save(\"artificial_images/artificial_image_\" + str(i) + \".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = torchvision.transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp_image = Image.new(\\'RGB\\', (IMAGE_WIDTH, IMAGE_HEIGHT), color=(0,0,0,0))\\nrandx = random.randint(0, IMAGE_WIDTH-32)\\nrandy = random.randint(0, IMAGE_HEIGHT-40)\\n\\nfor i in range(10):\\n    rand_number = random.randint(0, len(cropped_images)-1)\\n    temp_image.paste(to_pil(cropped_images[rand_number]), (randx, randy))\\n    temp_image = transform(temp_image)\\n    temp_image = draw_bounding_boxes(temp_image, cell_bbox[rand_number].unsqueeze(0), colors=\"red\", fill=\"False\")\\ntemp_image = torchvision.transforms.ToPILImage()(temp_image)\\ntemp_image.save(\"artificial_images/artificial_image_\" + str(i) + \".png\")'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"temp_image = Image.new('RGB', (IMAGE_WIDTH, IMAGE_HEIGHT), color=(0,0,0,0))\n",
    "randx = random.randint(0, IMAGE_WIDTH-32)\n",
    "randy = random.randint(0, IMAGE_HEIGHT-40)\n",
    "\n",
    "for i in range(10):\n",
    "    rand_number = random.randint(0, len(cropped_images)-1)\n",
    "    temp_image.paste(to_pil(cropped_images[rand_number]), (randx, randy))\n",
    "    temp_image = transform(temp_image)\n",
    "    temp_image = draw_bounding_boxes(temp_image, cell_bbox[rand_number].unsqueeze(0), colors=\"red\", fill=\"False\")\n",
    "temp_image = torchvision.transforms.ToPILImage()(temp_image)\n",
    "temp_image.save(\"artificial_images/artificial_image_\" + str(i) + \".png\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function all(iterable, /)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_boxes = torch.zeros(0)\n",
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 70., 48.],\n",
      "        [ 0.,  0., 70., 48.],\n",
      "        [ 0.,  0., 70., 48.],\n",
      "        [ 0.,  0., 32., 32.]])\n"
     ]
    }
   ],
   "source": [
    "print(cell_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140620020592128 140620020591008\n",
      "False\n",
      "tensor([ 0.,  0., 32., 33.]) tensor([ 0.,  0., 32., 33.])\n"
     ]
    }
   ],
   "source": [
    "temp_ting = torch.clone(cell_bbox[0])\n",
    "print(id(temp_ting), id(cell_bbox[0]))\n",
    "print(id(temp_ting) == id(cell_bbox[0]))\n",
    "print(temp_ting, cell_bbox[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0., 32., 33.])\n"
     ]
    }
   ],
   "source": [
    "print(temp_ting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0., 32., 33.])\n"
     ]
    }
   ],
   "source": [
    "print(cell_bbox[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140620020592128"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(temp_ting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140620020603776"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(cell_bbox[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 100 images with randomly selected cropped_images and cell_bbox converted to pytorch tensors\n",
    "# Use the draw_bounding_boxes function to draw the bounding box on the image\n",
    "# Save the image to a folder called \"artificial_images\"\n",
    "\n",
    "IMAGE_WIDTH = 696\n",
    "IMAGE_HEIGHT = 520\n",
    "\n",
    "boxes = []\n",
    "\n",
    "# Make 100 randomly generated images\n",
    "for i in range(100):\n",
    "    temp_image = Image.new('RGB', (IMAGE_WIDTH, IMAGE_HEIGHT), color=(0,0,0,0))\n",
    "    all_boxes = torch.zeros(0)\n",
    "    #print(\"Uno \" + str(type(temp_image)))\n",
    "    \n",
    "    # Paste 10 cropped_images onto temp_image\n",
    "    for j in range(10):\n",
    "\n",
    "        random.seed(random.randint(0, 100000))\n",
    "            \n",
    "        randx = random.randint(0, IMAGE_WIDTH-32)\n",
    "        randy = random.randint(0, IMAGE_HEIGHT-40)  \n",
    "\n",
    "        rand_number = random.randint(0, len(cropped_images)-1)\n",
    "\n",
    "        temp_image.paste(to_pil(cropped_images[rand_number]), (randx, randy))\n",
    "\n",
    "        # Add randx and randy to the cell_bbox coordinates\n",
    "        temp_box = torch.clone(cell_bbox[rand_number].unsqueeze(0))\n",
    "        #print(temp_box)\n",
    "        temp_box[:, 0] = temp_box[:, 0] + randx\n",
    "        temp_box[:, 1] = temp_box[:, 1] + randy\n",
    "        temp_box[:, 2] = temp_box[:, 2] + randx\n",
    "        temp_box[:, 3] = temp_box[:, 3] + randy\n",
    "\n",
    "        #print(temp_box[0, 3])\n",
    "\n",
    "        all_boxes = torch.cat((all_boxes, temp_box))\n",
    "\n",
    "        boxes.append(all_boxes)\n",
    "\n",
    "    #print(all_boxes)\n",
    "\n",
    "    #torch.reshape(all_boxes, (10, 4))\n",
    "\n",
    "    # enumerate through all_boxes\n",
    "    # for each box, add the x and y coordinates of the top left corner of the image\n",
    "    # to the x and y coordinates of the box\n",
    "    # then save the image\n",
    "\n",
    "    #print(all_boxes)\n",
    "\n",
    "    bbox_temp_image = transform(temp_image) \n",
    "    bbox_temp_image = draw_bounding_boxes(bbox_temp_image, all_boxes, colors=\"red\", fill=\"False\")\n",
    "\n",
    "    #print(i, str(type(temp_image)))\n",
    "\n",
    "    bbox_temp_image = torchvision.transforms.ToPILImage()(bbox_temp_image)\n",
    "    #temp_image = torchvision.transforms.ToPILImage()(temp_image)\n",
    "    temp_image.save(\"artificial_images/artificial_image_\" + str(i) + \".png\")\n",
    "    bbox_temp_image.save(\"artificial_bboxes/artificial_bbox_\" + str(i) + \".png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE\n"
     ]
    }
   ],
   "source": [
    "write_box_info_to_file(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 32., 33.],\n",
      "        [ 0.,  0., 70., 48.],\n",
      "        [ 0.,  0., 70., 48.],\n",
      "        [ 0.,  0., 70., 48.],\n",
      "        [ 0.,  0., 32., 32.]])\n"
     ]
    }
   ],
   "source": [
    "print(cell_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_box = cell_bbox[rand_number].unsqueeze(0)\n",
    "temp_box[:, 0] = temp_box[:, 0] + randx\n",
    "temp_box[:, 1] = temp_box[:, 1] + randy\n",
    "temp_box[:, 2] = temp_box[:, 2] + randx\n",
    "temp_box[:, 3] = temp_box[:, 3] + randy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [1., 2., 3., 4.],\n",
       "        [1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array = torch.zeros(0)\n",
    "test_array = torch.cat((test_array, torch.Tensor([1,2,3,4]).unsqueeze(0)))\n",
    "test_array = torch.cat((test_array, torch.Tensor([1,2,3,4]).unsqueeze(0)))\n",
    "test_array = torch.cat((test_array, torch.Tensor([1,2,3,4]).unsqueeze(0)))\n",
    "test_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([[151., 249., 183., 282.]])\n",
      "<class 'torch.Tensor'> tensor([446., 153., 478., 185.])\n"
     ]
    }
   ],
   "source": [
    "print(type(cell_bbox[rand_number].unsqueeze(0)), cell_bbox[rand_number].unsqueeze(0))\n",
    "print(type(all_boxes[0]), all_boxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEcAAAAxCAIAAAAHqSOUAAABJElEQVR4nO2awQ6EIAxEp8b//2X2QOISl0JF1HbWOSoJfUwtRZUEQq0ARLmXUjBkEQGQgEUbEQ4JRcwqVVBlsDpVRKNKVaiiI4EvA7P2VARG4S+84jAKJRUNEsgzkMkoMHvFZFRucHm9olE2CmRUm16qOOKh2h4qMFGVeqkcq0w/ZKrdJQIxePXryqLdCK2vV0HBqmEv3RERtX+uYoFp0VaqRSywquo1MARYI0i1sjsHa4fX2q/cgnUD6+zCIuKWrSFTb+EHzLjK1o7JD5hFB/rAx7PRPvvh7vYpsEPzrtfFMUsD6zhyErnTrrG5Bs9X94ANz+I0A0+u2vhZ+CK7plRaR15NXKZT7y1mxTF9J3zMq0vrjST9nzOjLB/17twMUqbi0wd+MynNngwy4gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=71x49>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Claca equals cropped_images[0] converted to a PIL image\n",
    "rand_number = random.randint(0, len(cropped_images)-1)\n",
    "\n",
    "claca = cropped_images[rand_number]\n",
    "print(type(claca))\n",
    "claca = draw_bounding_boxes(claca, cell_bbox[rand_number].unsqueeze(0), colors=\"red\")\n",
    "\n",
    "claca = torchvision.transforms.ToPILImage()(claca)\n",
    "claca.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArgAAAIICAIAAAAKaAoNAAAFnUlEQVR4nO3dS7KCMBAF0MZyq6yKxfoGWn7JEzWQD+dMmfSsb90QGE7RoaH0AADQh2NEjIln0zRtOcrvxnGMiMaGBoCKHVIPmksJ0ebMAFCzZFBolKwAABnNBwXrFgCI2aAgJQAAZ70dPQAAGT0HBXUCAHClUQAAkh6CgjoBALh3CwpSAgDwxNEDAJB0CQrqBADglUYBAEg6RF91wvm/UABAFhoFACCpq6CgTgCAvLoKCgBAXoICAJAkKAAASf0EBS8oAEB2/QQFACA7QQEASOokKDh3AIA1HMKWBQASemgUBB0AWMklKNi1AMCrW6PQaFZodGwAaMLD0YOlCwDce35Hoa2s0Na0ANCcmZcZbV8A4Gz+1kMTWaGJIQGgacnrkZWv4crHA4A+/PcdhWqXcbWDAUBn3nxwaRxHWxkAdms4RSwJAtM0rT7LAktSyxQxbDAKAOzA0k846xUAYIeWNgpXBauFhWFFowAAuXz8U6hS1YJKAwC2dyw9wHsiAgCU8s1vprfc3FICABT0TVCIrfa3lAAAZVV69CAiAEANPr71cG+NGxC/RwS3HgAgl4oaBS0CANTmp0YhMpUKeSOCRgEAcinWKOgPAKB+vzYKsaxU2DIWaBQAIJfhVHqCNQgKAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwH78AaTUV3CIcwVDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=696x520>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blaka = Image.new('RGB', (IMAGE_WIDTH, IMAGE_HEIGHT), color=(0,0,0,0))\n",
    "blaka_image = torchvision.transforms.ToPILImage()(cropped_images[7])\n",
    "print(type(blaka_image))\n",
    "blaka.paste(blaka_image, (0, 0))\n",
    "print(type(blaka))\n",
    "# Convert blaka to a pytorch tensor\n",
    "blaka = transform(blaka)\n",
    "\n",
    "blaka = draw_bounding_boxes(blaka, cell_bbox[7].unsqueeze(0), colors=\"red\", fill=\"False\")\n",
    "blaka = torchvision.transforms.ToPILImage()(blaka)\n",
    "blaka.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0., 32., 33.]])\n"
     ]
    }
   ],
   "source": [
    "print(cell_bbox[random.randint(0, len(cropped_images)-1)].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new folder called \"artificial_images\"\n",
    "# Create a new folder called \"artificial_images/annotations\"\n",
    "# Create a new folder called \"artificial_images/images\"\n",
    "\n",
    "# Create a new file called \"artificial_images/annotations/annotations.csv\"\n",
    "# Write the header \"image_name,x_min,y_min,x_max,y_max\" to the file\n",
    "\n",
    "# Create a new file called \"artificial_images/images/images.csv\"\n",
    "# Write the header \"image_name\" to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(100):\\n    rand_num = random.randint(0, 9)\\n    temp_image = cropped_images[rand_num]\\n    temp_box = cell_bbox[rand_num]\\n\\n    temp_image = draw_bounding_boxes(temp_image, temp_box, width=0, colors=\"red\", fill=\"False\")\\n    temp_image = torchvision.transforms.ToPILImage()(temp_image)\\n    temp_image.save(\"artificial_images/\" + str(i) + \".png\")'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 100 images with randomly selected cropped_images and cell_bbox\n",
    "# Use the draw_bounding_boxes function to draw the bounding box on the image\n",
    "# Save the image to a folder called \"artificial_images\"\n",
    "\n",
    "\"\"\"for i in range(100):\n",
    "    rand_num = random.randint(0, 9)\n",
    "    temp_image = cropped_images[rand_num]\n",
    "    temp_box = cell_bbox[rand_num]\n",
    "\n",
    "    temp_image = draw_bounding_boxes(temp_image, temp_box, width=0, colors=\"red\", fill=\"False\")\n",
    "    temp_image = torchvision.transforms.ToPILImage()(temp_image)\n",
    "    temp_image.save(\"artificial_images/\" + str(i) + \".png\")\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('YOLOv5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59c369f0353834940d06bc12af3d8934cf452b2253cb1a3d912f389c93b40c3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
